{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlvdpFsgdy9sAxd2sfPHxr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushp0119/Pushp-UCS420/blob/main/ASSIGNMENT9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUb6KGOp5P25",
        "outputId": "756a890e-6490-4779-9db3-b647bf561d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.8.1\n",
        "\n",
        "import nltk\n",
        "print(nltk.__version__)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "hyUQ4JAi52pH",
        "outputId": "24f279fb-2bb9-45da-c24d-64ad4b596a8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (4.67.1)\n",
            "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m1.2/1.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nltk-3.8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              },
              "id": "8134d2a87dd7414897b133455a056ec2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
        "technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuaƟon.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribuƟon (excluding stopwords)."
      ],
      "metadata": {
        "id": "Y080Y_Fw6E6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Technology is evolving rapidly. I love exploring state-of-the-art innovations. Artificial Intelligence (AI), blockchain, and robotics fascinate me. These technologies have the power to transform industries. With each passing year, devices are becoming smarter and more connected.\"\n",
        "\n",
        "text_clean = re.sub(r'[^\\w\\s\\-]', '', text.lower())\n",
        "\n",
        "words = word_tokenize(text_clean)\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "word_freq = Counter(filtered_words)\n",
        "\n",
        "print(text_clean)\n",
        "print()\n",
        "print(words)\n",
        "print()\n",
        "print(sentences)\n",
        "print()\n",
        "print(filtered_words)\n",
        "print()\n",
        "print(word_freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqzjZ0Uj6mJX",
        "outputId": "7ba695eb-b92f-4d78-8658-2fd0fdd32e17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "technology is evolving rapidly i love exploring state-of-the-art innovations artificial intelligence ai blockchain and robotics fascinate me these technologies have the power to transform industries with each passing year devices are becoming smarter and more connected\n",
            "\n",
            "['technology', 'is', 'evolving', 'rapidly', 'i', 'love', 'exploring', 'state-of-the-art', 'innovations', 'artificial', 'intelligence', 'ai', 'blockchain', 'and', 'robotics', 'fascinate', 'me', 'these', 'technologies', 'have', 'the', 'power', 'to', 'transform', 'industries', 'with', 'each', 'passing', 'year', 'devices', 'are', 'becoming', 'smarter', 'and', 'more', 'connected']\n",
            "\n",
            "['Technology is evolving rapidly.', 'I love exploring state-of-the-art innovations.', 'Artificial Intelligence (AI), blockchain, and robotics fascinate me.', 'These technologies have the power to transform industries.', 'With each passing year, devices are becoming smarter and more connected.']\n",
            "\n",
            "['technology', 'evolving', 'rapidly', 'love', 'exploring', 'state-of-the-art', 'innovations', 'artificial', 'intelligence', 'ai', 'blockchain', 'robotics', 'fascinate', 'technologies', 'power', 'transform', 'industries', 'passing', 'year', 'devices', 'becoming', 'smarter', 'connected']\n",
            "\n",
            "Counter({'technology': 1, 'evolving': 1, 'rapidly': 1, 'love': 1, 'exploring': 1, 'state-of-the-art': 1, 'innovations': 1, 'artificial': 1, 'intelligence': 1, 'ai': 1, 'blockchain': 1, 'robotics': 1, 'fascinate': 1, 'technologies': 1, 'power': 1, 'transform': 1, 'industries': 1, 'passing': 1, 'year': 1, 'devices': 1, 'becoming': 1, 'smarter': 1, 'connected': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Stemming and LemmaƟzaƟon\n",
        "1. Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
        "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3. Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
        "4. Compare and display results of both techniques."
      ],
      "metadata": {
        "id": "oR4bMN8i6yZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "porter_stems = [porter.stem(word) for word in filtered_words]\n",
        "lancaster_stems = [lancaster.stem(word) for word in filtered_words]\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "print(porter_stems)\n",
        "print(lancaster_stems)\n",
        "print(lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfIlInNQ6zO_",
        "outputId": "b9ecdcd7-e522-46f9-fc6a-47811a1ea677"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['technolog', 'evolv', 'rapidli', 'love', 'explor', 'state-of-the-art', 'innov', 'artifici', 'intellig', 'ai', 'blockchain', 'robot', 'fascin', 'technolog', 'power', 'transform', 'industri', 'pass', 'year', 'devic', 'becom', 'smarter', 'connect']\n",
            "['technolog', 'evolv', 'rapid', 'lov', 'expl', 'state-of-the-art', 'innov', 'art', 'intellig', 'ai', 'blockchain', 'robot', 'fascin', 'technolog', 'pow', 'transform', 'industry', 'pass', 'year', 'dev', 'becom', 'smart', 'connect']\n",
            "['technology', 'evolving', 'rapidly', 'love', 'exploring', 'state-of-the-art', 'innovation', 'artificial', 'intelligence', 'ai', 'blockchain', 'robotics', 'fascinate', 'technology', 'power', 'transform', 'industry', 'passing', 'year', 'device', 'becoming', 'smarter', 'connected']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Regular Expressions and Text Spliƫng\n",
        "1. Take their original text from QuesƟon 1.\n",
        "2. Use regular expressions to:\n",
        "a. Extract all words with more than 5 leƩers.\n",
        "b. Extract all numbers (if any exist in their text).\n",
        "c. Extract all capitalized words.\n",
        "3. Use text spliƫng techniques to:\n",
        "a. Split the text into words containing only alphabets (removing digits and special\n",
        "characters).\n",
        "b. Extract words starƟng with a vowel."
      ],
      "metadata": {
        "id": "8gslcQ_r67QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_text = text\n",
        "\n",
        "long_words = re.findall(r'\\b\\w{6,}\\b', original_text)\n",
        "numbers = re.findall(r'\\b\\d+\\b', original_text)\n",
        "capitalized = re.findall(r'\\b[A-Z][a-z]*\\b', original_text)\n",
        "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', original_text)\n",
        "vowel_words = [word for word in alpha_words if word.lower().startswith(('a', 'e', 'i', 'o', 'u'))]\n",
        "\n",
        "print(long_words)\n",
        "print(numbers)\n",
        "print(capitalized)\n",
        "print(alpha_words)\n",
        "print(vowel_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrWhB-9E68Lz",
        "outputId": "4e769a49-62ff-4292-b8b1-9cea4d7e6f6d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Technology', 'evolving', 'rapidly', 'exploring', 'innovations', 'Artificial', 'Intelligence', 'blockchain', 'robotics', 'fascinate', 'technologies', 'transform', 'industries', 'passing', 'devices', 'becoming', 'smarter', 'connected']\n",
            "[]\n",
            "['Technology', 'I', 'Artificial', 'Intelligence', 'These', 'With']\n",
            "['Technology', 'is', 'evolving', 'rapidly', 'I', 'love', 'exploring', 'state', 'of', 'the', 'art', 'innovations', 'Artificial', 'Intelligence', 'AI', 'blockchain', 'and', 'robotics', 'fascinate', 'me', 'These', 'technologies', 'have', 'the', 'power', 'to', 'transform', 'industries', 'With', 'each', 'passing', 'year', 'devices', 'are', 'becoming', 'smarter', 'and', 'more', 'connected']\n",
            "['is', 'evolving', 'I', 'exploring', 'of', 'art', 'innovations', 'Artificial', 'Intelligence', 'AI', 'and', 'industries', 'each', 'are', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
        "1. Take original text from QuesƟon 1.\n",
        "2. Write a custom tokenizaƟon funcƟon that:\n",
        "a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\n",
        "\"isn't\" should not be split into \"is\" and \"n't\").\n",
        "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
        "a single token).\n",
        "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
        "should remain as is).\n",
        "3. Use Regex SubsƟtuƟons (re.sub) to:\n",
        "a. Replace email addresses with '<EMAIL>' placeholder.\n",
        "b. Replace URLs with '<URL>' placeholder.\n",
        "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
        "'<PHONE>' placeholder."
      ],
      "metadata": {
        "id": "1AzmKE6Z7G4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_tokenizer(text):\n",
        "    text = re.sub(r\"(?!\\b[\\w'-]+\\b)[^\\w\\s.-]\", '', text)\n",
        "    tokens = re.findall(r\"\\b\\w+(?:[-.]\\w+)*\\b\", text)\n",
        "    return tokens\n",
        "\n",
        "tokens_custom = custom_tokenizer(text)\n",
        "\n",
        "text_sub = text\n",
        "text_sub = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '', text_sub)\n",
        "text_sub = re.sub(r'https?://\\S+|www\\.\\S+', '', text_sub)\n",
        "text_sub = re.sub(r'\\b(?:\\+91[-\\s]?)?\\d{10}\\b|\\d{3}[-\\s]?\\d{3}[-\\s]?\\d{4}', '', text_sub)\n",
        "\n",
        "print(tokens_custom)\n",
        "print(text_sub)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF9cRyHg7KmQ",
        "outputId": "7286fec1-5886-4a63-cbef-83dfaa2c4dee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Technology', 'is', 'evolving', 'rapidly', 'I', 'love', 'exploring', 'state-of-the-art', 'innovations', 'Artificial', 'Intelligence', 'AI', 'blockchain', 'and', 'robotics', 'fascinate', 'me', 'These', 'technologies', 'have', 'the', 'power', 'to', 'transform', 'industries', 'With', 'each', 'passing', 'year', 'devices', 'are', 'becoming', 'smarter', 'and', 'more', 'connected']\n",
            "Technology is evolving rapidly. I love exploring state-of-the-art innovations. Artificial Intelligence (AI), blockchain, and robotics fascinate me. These technologies have the power to transform industries. With each passing year, devices are becoming smarter and more connected.\n"
          ]
        }
      ]
    }
  ]
}